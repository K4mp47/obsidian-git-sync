---
title: "Mattia Piazzon - Esercizio 2"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

# Quiz 1
Dati $X_1 ... X_{261}$ con
$$
X \sim
\begin{cases}
\frac\theta2, & \text{se }x = -1,\\
1-\theta, & \text{se }x = 0,\\
\frac\theta2, & \text{se }x = 1.\\
\end{cases}
$$

## Quesito 1: stima di $\theta$ via momenti
Utilizzando il momento di ordine 1 si ottiene
$$
\mu_1 = E(X) = 0
$$
Che non permette di stimare $\theta$. Utilizzando il momento di ordine 2 si ottiene
$$
\mu_2 = E(X^2) = \theta
$$
Che permette di stimare $\theta$ come
$$
\begin{cases}
\mu_2 = M_2 \Rightarrow \hat\theta = \frac1n\sum_{i=1}^n X_i^2
\end{cases}
$$

Stima:
$\hat\theta =$
```{r, error = TRUE}
(109 + 103) / 261
```


## Quesito 2: stima di $\theta$ via verosimiglianza

La verosimiglianza per $\theta$ è
$$
L(\theta) = \prod_{i=1}^n f(x_i) = \Big(\frac\theta2\Big)^{212} \cdot \big(1 -\theta\big)^{49}
$$
La cui log-verosimiglianza è
$$
\ell(\theta) = \log L(\theta) = 212\log\frac\theta2 + 49\log(1-\theta)
$$
Ponendo la derivata prima uguale a zero si ottiene
$$
\ell'(\hat\theta) = 0 \Rightarrow \frac{212}{\hat\theta} - \frac{49}{1-\hat\theta} = 0 \Rightarrow \hat\theta = \frac{212}{212+49}
$$
```{r, error = TRUE}
t <- 212 / (212 + 49)
t
```
Che coincide con la massima verosimiglianza poiche
$$
\ell''(\hat\theta) = -\frac{212}{\hat\theta^2} - \frac{49}{(1-\hat\theta)^2} < 0 \quad \forall \hat\theta \in (0,1)
$$

## Quesito 3: stima di $SE(\hat\theta)$

Utilizzando l'informazione osservata di Fisher:
$$
J(\hat\theta) = -\ell''(\hat\theta) = \frac{212}{\hat\theta^2} + \frac{49}{(1-\hat\theta)^2}
$$
Calcolandola in $\hat\theta$ si ottiene
```{r, error = TRUE}
J <- 212 / t^2 + 49 / (1 - t)^2
J
```
Quindi l'error standard di $\hat\theta$ è
$$
\hat{SE}(\hat\theta) = \sqrt{\frac1{J(\hat\theta)}}
$$
```{r, error = TRUE}
sqrt(1 / J)
```

# Quiz 2
Dati $X_1 ... X_{261}$ con $X \sim \frac1\theta x^{(1 - \theta) / \theta}$ e $0 < x < 1$ e $\theta > 0$

## Quesito 1: stima di $\theta$ via momenti
Utilizzando il momento di ordine 1 si ottiene
$$
\mu_1 = E(X) = \int_0^1 x \frac1\theta x^{(1 - \theta) / \theta} dx =
\frac1{1 + \theta}
$$
Che permette di stimare $\theta$ come
$$
\begin{cases}
\mu_1 = M_1 \Rightarrow \frac1{1 + \theta} = \bar{X} \Rightarrow \hat\theta = \frac1{\bar{X}} - 1
\end{cases}

Stima:
dato the $\bar{X} = \frac1{261} \sum_{i=1}^{261} X_i = \frac{131.4}{261}$ si ha che $\hat\theta = \frac{261}{131.4} - 1$
```{r, error = TRUE}
261 / 131.4 - 1
```

## Quesito 2: stima di $\theta$ via verosimiglianza

La verosimiglianza per $\theta$ è
$$
L(\theta) = \prod_{i=1}^n f(x_i) = \prod_{i=1}^n \frac1\theta x_i^{(1 - \theta) / \theta}
$$

La cui log-verosimiglianza è
$$
\ell(\theta) = \log L(\theta) = -n\log\theta + \frac{1 - \theta}{\theta} \sum_{i=1}^n \log x_i = -n\log\theta + \frac1\theta \sum_{i=1}^n \log x_i - \sum_{i=1}^n \log x_i
$$

Ponendo la derivata prima uguale a zero si ottiene
$$
\ell'(\hat\theta) = 0 \Rightarrow -\frac n{\hat\theta} - \frac 1 {\hat\theta^2}\sum_{i=1}^n \log X_i = 0 \Rightarrow \hat\theta = -\frac 1n\sum_{i=1}^n \log X_i
$$
Che coincide con la massima verosimiglianza poiche la derivata seconda
$$
\ell''(\hat\theta) = \frac n{\hat\theta^2} + \frac 2{\hat\theta^3}\sum_{i=1}^n \log X_i
$$
calcolata in $\hat\theta$ è negativa

Stima:
dato che $\sum_{i=1}^{261} \log X_i = -159.59$ si ha che $\hat\theta = -\frac{-159.59}{261}$
```{r, error = TRUE}
t <- - (-159.59) / 261
t
```

## Quesito 3: stima di $SE(\hat\theta)$

Utilizzando l'informazione osservata di Fisher:
$$
J(\hat\theta) = -\ell''(\hat\theta) = -\frac n{\hat\theta^2} - \frac 2{\hat\theta^3}\sum_{i=1}^n \log X_i
$$
Calcolandola in $\hat\theta$ si ottiene
```{r, error = TRUE}
J <- -261 / t^2 - 2 / t^3 * (-159.59)
J
```
Quindi l'error standard di $\hat\theta$ è
$$
\hat{SE}(\hat\theta) = \sqrt{\frac1{J(\hat\theta)}}
$$
```{r, error = TRUE}
sqrt(1 / J)
```

# Quiz 3
Dati $X_1 ... X_{60}$ con $X \sim N(0, \sigma^2)$

## Quesito 1: stima di $\sigma^2$ via momenti
Utilizzando il momento di ordine 1 si ottiene
$$
\mu_1 = E(X) = 0
$$
Che non permette di stimare $\sigma^2$. Utilizzando il momento di ordine 2 si ottiene
$$
\mu_2 = E(X^2) = Var(X) + E(X)^2 = \sigma^2
$$
Che permette di stimare $\sigma^2$ come
$$
\begin{cases}
\mu_2 = M_2 \Rightarrow \hat\sigma^2 = \frac1n\sum_{i=1}^n X_i^2
\end{cases}
$$
Stima:
dato che $\sum_{i=1}^{60} X_i^2 = 164.83$ si ha che $\hat\sigma^2 = \frac{164.83}{60}$
```{r, error = TRUE}
164.83 / 60
```

## Quesito 2: stima di $\sigma^2$ via verosimiglianza e errore standard
La verosimiglianza per $\sigma^2$ è
$$
L(\sigma^2) = \prod_{i=1}^n f(x_i) = \prod_{i=1}^n \frac1{\sqrt{2\pi\sigma^2}} e^{-\frac{x_i^2}{2\sigma^2}}
$$
La cui log-verosimiglianza è
$$
\ell(\sigma^2) = \log L(\sigma^2) = \sum_{i=1}^n \log(f(x_i)) = -\frac n2\log(2\pi) - \frac n2\log\sigma^2 - \frac1{2\sigma^2}\sum_{i=1}^n X_i^2
$$
Ponendo la derivata prima uguale a zero si ottiene
$$
\ell'(\hat\sigma^2) = 0 \Rightarrow -\frac n{2\hat\sigma^2} + \frac1{2\hat\sigma^4}\sum_{i=1}^n X_i^2 = 0 \Rightarrow \hat\sigma^2 = \frac1n\sum_{i=1}^n X_i^2
$$
Che coincide con la massima verosimiglianza poiche la derivata seconda
$$
\ell''(\hat\sigma^2) = \frac n{2\hat\sigma^4} - \frac1{\hat\sigma^6}\sum_{i=1}^n X_i^2
$$
calcolata in $\hat\sigma^2$ è negativa.

Stima:
dato che $\sum_{i=1}^{60} X_i^2 = 164.83$ si ha che $\hat\sigma^2 = \frac{164.83}{60}$
```{r, error = TRUE}
t <- 164.83 / 60
t
```

Per stimare l'errore standard di $\hat\sigma^2$ si calcola l'informazione osservata di Fisher
$$
J(\hat\sigma^2) = -\ell''(\hat\sigma^2) = -\frac n{2\hat\sigma^4} + \frac1{\hat\sigma^6}\sum_{i=1}^n X_i^2
$$
Calcolandola in $\hat\sigma^2$ si ottiene
```{r, error = TRUE}
J <- -60 / (2 * t^2) + 1 / t^3 * 164.83
J
```
Quindi l'error standard di $\hat\sigma^2$ è
$$
\hat{SE}(\hat\sigma^2) = \sqrt{\frac1{J(\hat\sigma^2)}}
$$
```{r, error = TRUE}
se <- sqrt(1 / J)
se
```

## Quesito 3: stima di $\hat\sigma$ via verosimiglianza e errore standard
Poiche $\hat\sigma = \sqrt{\hat\sigma^2}$ con $g(x) = \sqrt{x}$
, per invarianza della massima verosimiglianza si ha che
$$
\hat\sigma = \sqrt{\frac1n\sum_{i=1}^n X_i^2}
$$
che stima $\sigma$ come
```{r, error = TRUE}
sqrt(t)
```

Per stimare l'errore standard di $\hat\sigma$ sappiamo che
$$
\hat{SE}(\hat\sigma) \approx g'(\hat\sigma) \hat{SE}(\hat\sigma^2) = \frac1{2\sqrt{\hat\sigma^2}} \hat{SE}(\hat\sigma^2)
$$
```{r, error = TRUE}
se / (2 * sqrt(t))
```

# Quiz 4

Dati $X_1 ... X_{52}$ con $X \sim \theta(1-\theta)^x$ e $x = 0...$ e $0 < \theta < 1$

## Quesito 1: stima di $\theta$ via verosimiglianza

La verosimiglianza per $\theta$ è
$$
L(\theta) = \prod_{i=1}^n f(x_i) = \prod_{i=1}^n \theta(1-\theta)^{x_i}
$$

La cui log-verosimiglianza è
$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log(f(x_i)) = \sum_{i=1}^n \log(\theta) + x_i\log(1-\theta)
$$

Ponendo la derivata prima uguale a zero si ottiene
$$
\ell'(\hat\theta) = 0 \Rightarrow \frac n{\hat\theta} - \frac1{1-\hat\theta}\sum_{i=1}^n x_i = 0 \Rightarrow \hat\theta = \frac{n}{n + \sum_{i=1}^n x_i}
$$

Che coincide con la massima verosimiglianza poiche la derivata seconda
$$
\ell''(\hat\theta) = -\frac n{\hat\theta^2} - \frac1{(1-\hat\theta)^2}\sum_{i=1}^n x_i
$$
è sempre negativa $\forall \hat\theta \in (0,1)$

Stima:
dato che $\sum_{i=1}^{52} x_i = 31$ si ha che $\hat\theta = \frac{52}{52 + 31}$
```{r, error = TRUE}
t <- 52 / (52 + 31)
t
```

## Quesito 2: stima di $SE(\hat\theta)$

Utilizzando l'informazione osservata di Fisher:
$$
J(\hat\theta) = -\ell''(\hat\theta) = \frac n{\hat\theta^2} + \frac1{(1-\hat\theta)^2}\sum_{i=1}^n x_i
$$

Calcolandola in $\hat\theta$ si ottiene
```{r, error = TRUE}
J <- 52 / t^2 + 31 / (1 - t)^2
J
```

Quindi l'error standard di $\hat\theta$ è
$$
\hat{SE}(\hat\theta) = \sqrt{\frac1{J(\hat\theta)}}
$$
```{r, error = TRUE}
se <- sqrt(1 / J)
se
```

## Quesito 3: stima di $\psi = log(\theta)$ via verosimiglianza e errore standard

Poiche $\psi = log(\theta)$ con $g(x) = log(x)$, per invarianza della massima verosimiglianza si ha che
$$
\hat\psi = log(\hat\theta) = log\Big(\frac{n}{n + \sum_{i=1}^n x_i}\Big) = log(n) - log(n + \sum_{i=1}^n x_i)
$$

Che stima $\psi$ come
```{r, error = TRUE}
log(52) - log(52 + 31)
```

Per stimare l'errore standard di $\hat\psi$ sappiamo che
$$
\hat{SE}(\hat\psi) \approx g'(\hat\psi) \hat{SE}(\hat\theta) = \frac1{\hat\theta} \hat{SE}(\hat\theta)
$$

```{r, error = TRUE}
se / t
```

